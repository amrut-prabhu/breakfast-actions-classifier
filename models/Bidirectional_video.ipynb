{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import _pickle as pickle\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "import time\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = \"../\"\n",
    "\n",
    "DATA_DIR = ROOT_DIR\n",
    "RESULTS_DIR = ROOT_DIR + \"results/\"\n",
    "\n",
    "# Train - 1460 (1168 + 292) videos \n",
    "# Test - 252 videos \n",
    "\n",
    "TRAINING_DATA = DATA_DIR + \"video_training_data.p\" # each read gives labelled video [(segment, label), segment_2, ...]\n",
    "VALIDATION_DATA = DATA_DIR + \"video_validation_data.p\" \n",
    "TEST_DATA = DATA_DIR + \"video_testing_data.p\" # each read gives video [(segment), segment_2, ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_video_data(f):    \n",
    "    is_end_reached = False\n",
    "    \n",
    "    try:\n",
    "        video = pickle.load(f) # [(segment, label), ...]\n",
    "    except (EOFError):\n",
    "        is_end_reached = True\n",
    "        return [], is_end_reached\n",
    "    \n",
    "    return video, is_end_reached"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Temporal Model (RNN, LSTM, GRU):**\n",
    "- [ ] Add dropout\n",
    "- [ ] Add stacking / another lstm\n",
    "- [ ] No temporal pooling\n",
    "- [ ] Max pooling\n",
    "- [ ] Avg pooling\n",
    "- [ ] [Concat pooling](https://medium.com/@sonicboom8/sentiment-analysis-with-variable-length-sequences-in-pytorch-6241635ae130)\n",
    "- [ ] CS4248 12-rnn slide 32: Use pooled hidden states (segment level) in another RNN, and predict for each resultatnt hidden state. \n",
    "\n",
    "**Try out models:**\n",
    "- [ ] RNN\n",
    "- [ ] GRU\n",
    "- [ ] [QRNN](https://github.com/salesforce/pytorch-qrnn)\n",
    "- [ ] Nested LSTM\n",
    "- [ ] TCN\n",
    "\n",
    "\n",
    "[LSTM vs GRU](https://blog.floydhub.com/gru-with-pytorch/) explanation and comparison article\n",
    "\n",
    "\n",
    "**Training:**\n",
    "- [ ] Shuffle data\n",
    "- [ ] Xavier normal initialisation\n",
    "- [ ] Xavier uniform initialisation\n",
    "\n",
    "\n",
    "Yash- high validation accuracy for **hidden dim 50**. But test results are bad.\n",
    "\n",
    "\n",
    "Don't use the SIL frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassPredictor(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(ClassPredictor, self).__init__()\n",
    "        \n",
    "        hidden_1 = 160\n",
    "        hidden_2 = 100\n",
    "        \n",
    "#         self.temp = nn.Linear(input_size, num_classes)\n",
    "        \n",
    "        # https://pytorch.org/docs/stable/nn.html#torch.nn.Linear\n",
    "        self.fc1 = nn.Linear(input_size, hidden_1)\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=hidden_1)\n",
    "        self.l_relu1 = nn.LeakyReLU()\n",
    "        self.dout1 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        self.bn2 = nn.BatchNorm1d(num_features=hidden_2)\n",
    "        self.l_relu2 = nn.LeakyReLU()\n",
    "        \n",
    "        self.out = nn.Linear(hidden_2, num_classes)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            nn.init.xavier_uniform_(self.fc1.weight, gain=nn.init.calculate_gain('leaky_relu'))\n",
    "            nn.init.xavier_uniform_(self.fc2.weight, gain=nn.init.calculate_gain('leaky_relu'))\n",
    "            nn.init.xavier_uniform_(self.out.weight)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        ## x: (input_size)\n",
    "#         return self.temp(x)\n",
    "        \n",
    "        a1 = self.fc1(x)\n",
    "#         b1 = self.bn1(a1)\n",
    "        h1 = self.l_relu1(a1)\n",
    "        dout1 = self.dout1(h1)\n",
    "\n",
    "        a2 = self.fc2(dout1)\n",
    "#         b2 = self.bn2(a2)\n",
    "        h2 = self.l_relu2(a2)\n",
    "        \n",
    "        # y: (num_classes)\n",
    "        y = self.out(h2)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bidirectional recurrent neural network (many-to-one)\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        drop_prob = 0\n",
    "        \n",
    "        ## https://pytorch.org/docs/stable/nn.html#torch.nn.LSTM\n",
    "        self.bilstm = nn.LSTM(input_size, hidden_size, num_layers, bidirectional=True, \n",
    "                            batch_first=True, bias=True, dropout=drop_prob)\n",
    "        \n",
    "        ## DNN for class prediction\n",
    "        self.fc = ClassPredictor(hidden_size * 2, num_classes)\n",
    " \n",
    "\n",
    "    ## https://pytorch.org/docs/stable/nn.init.html\n",
    "    def init_hidden_state(self, batch_size):\n",
    "        #h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size)\n",
    "        #c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size)\n",
    "\n",
    "        h0 = torch.empty(self.num_layers * 2, batch_size, self.hidden_size).double()\n",
    "        h0 = nn.init.orthogonal_(h0) # orthogonal_, xavier_normal_, xavier_uniform_\n",
    "        \n",
    "        c0 = torch.empty(self.num_layers * 2, batch_size, self.hidden_size).double()\n",
    "        c0 = nn.init.orthogonal_(c0) # orthogonal_, xavier_normal_, xavier_uniform_\n",
    "\n",
    "        h0 = h0.requires_grad_().cuda()\n",
    "        c0 = c0.requires_grad_().cuda()\n",
    "        \n",
    "        return h0, c0\n",
    " \n",
    "\n",
    "    def forward(self, x, segment_indices):\n",
    "        ## x: (batch_size, seq_len, feature_len)\n",
    "        ## segment_indices: (num_segments, 2)\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        ## Set initial states\n",
    "        ## h0, c0: (num_layers * num_directions, batch_size, hidden_size)\n",
    "        h0, c0 = self.init_hidden_state(batch_size)\n",
    "        \n",
    "        ## Forward propagate LSTM\n",
    "        ## out: tensor of shape (batch_size, seq_length, hidden_size * 2)\n",
    "        out, _ = self.bilstm(x, (h0, c0))  \n",
    "        \n",
    "        ## Use hidden states of each segment to predict their labels\n",
    "        segment_outputs = []\n",
    "        for (start, end) in segment_indices:\n",
    "            hidden_states = out[:, start:end, :]\n",
    "            \n",
    "            ## Compute the hidden state by doing temporal pooling over all time steps\n",
    "            ## pool_out: (hidden_size * 2)\n",
    "            \n",
    "            pool_out = torch.mean(hidden_states, dim=1).squeeze() # adaptive_avg_pool1d\n",
    "#             pool_out = F.adaptive_max_pool1d(hidden_states.permute(0,2,1), 1).squeeze()\n",
    "\n",
    "            ## concat_pool_out: (hidden_size * 2 * 2)\n",
    "#             concat_pool_out = torch.cat([max_pool_out, avg_pool_out])\n",
    "\n",
    "#             final_hidden_out = out[:, end - 1, :]\n",
    "            \n",
    "            ## output: (num_classes)\n",
    "            output = self.fc(pool_out)\n",
    "            \n",
    "            segment_outputs.append(output)\n",
    "\n",
    "        \n",
    "        ## segment_outputs: (num_segments, num_classes)\n",
    "        segment_outputs = torch.stack(segment_outputs)\n",
    "        \n",
    "        return segment_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(BiGRU, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        drop_prob = 0\n",
    "        \n",
    "        ## https://pytorch.org/docs/stable/nn.html#torch.nn.GRU\n",
    "        self.bigru =nn.GRU(input_size, hidden_size, num_layers, bidirectional=True, \n",
    "                            batch_first=True, bias=True, dropout=drop_prob)\n",
    "        \n",
    "        ## DNN for class prediction\n",
    "        self.fc = ClassPredictor(hidden_size * 2 * 2, num_classes)\n",
    " \n",
    "\n",
    "    ## https://pytorch.org/docs/stable/nn.init.html\n",
    "    def init_hidden_state(self, batch_size):\n",
    "        #h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size)\n",
    "\n",
    "        h0 = torch.empty(self.num_layers * 2, batch_size, self.hidden_size).double()\n",
    "        h0 = nn.init.orthogonal_(h0) # orthogonal_, xavier_normal_, xavier_uniform_\n",
    "        h0 = h0.requires_grad_().cuda()\n",
    "        \n",
    "        return h0\n",
    " \n",
    "\n",
    "    def forward(self, x, segment_indices):\n",
    "        ## x: (batch_size, seq_len, feature_len)\n",
    "        ## segment_indices: (num_segments, 2)\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        ## Set initial states\n",
    "        ## h0: (num_layers * num_directions, batch_size, hidden_size)\n",
    "        h0 = self.init_hidden_state(batch_size)\n",
    "        \n",
    "        ## Forward propagate\n",
    "        ## out: tensor of shape (batch_size, seq_length, hidden_size * 2)\n",
    "        out, _ = self.bigru(x, h0)  \n",
    "        \n",
    "        ## Use hidden states of each segment to predict their labels\n",
    "        segment_outputs = []\n",
    "        for (start, end) in segment_indices:\n",
    "            hidden_states = out[:, start:end, :]\n",
    "            \n",
    "            ## Compute the hidden state by doing temporal pooling over all time steps\n",
    "            ## pool_out: (hidden_size * 2)\n",
    "            max_pool_out = F.adaptive_max_pool1d(hidden_states.permute(0,2,1), 1).squeeze()\n",
    "            avg_pool_out = torch.mean(hidden_states, dim=1).squeeze() # adaptive_avg_pool1d\n",
    "\n",
    "            ## concat_pool_out: (hidden_size * 2 * 2)\n",
    "            concat_pool_out = torch.cat([max_pool_out, avg_pool_out])\n",
    "\n",
    "#             final_hidden_out = out[:, end - 1, :]\n",
    "            \n",
    "            ## output: (num_classes)\n",
    "            output = self.fc(concat_pool_out)\n",
    "            \n",
    "            segment_outputs.append(output)\n",
    "\n",
    "        \n",
    "        ## segment_outputs: (num_segments, num_classes)\n",
    "        segment_outputs = torch.stack(segment_outputs)\n",
    "        \n",
    "        return segment_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden_states = v[:, 0:3, :]\n",
    "\n",
    "# max_pool = F.adaptive_max_pool1d(hidden_states.permute(0,2,1), 1).squeeze()\n",
    "\n",
    "# avg_pool = F.adaptive_avg_pool1d(hidden_states.permute(0,2,1), 1).squeeze()\n",
    "# a_pool = torch.mean(hidden_states, dim=1).squeeze()\n",
    "\n",
    "# a_pool\n",
    "# avg_pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model on entire videos (sequence of segments which are sequence of frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_inputs(video):\n",
    "    segments = [] # segments (list of frames) in the video\n",
    "    labels = [] # labels of each segment\n",
    "    segment_indices = []\n",
    "    offset = 0\n",
    "    for segment_num in range(len(video)):\n",
    "        segments.append(video[segment_num][0])\n",
    "        labels.append(video[segment_num][1])\n",
    "        segment_indices.append((offset, offset + video[segment_num][0].shape[0]))\n",
    "\n",
    "        offset += video[segment_num][0].shape[0]\n",
    "        \n",
    "    # Load frames as tensors with gradient accumulation abilities\n",
    "    input_frames = torch.cat(segments, 0).unsqueeze(0).requires_grad_().cuda() # unsqueeze to add batch dim\n",
    "    labels = torch.Tensor(labels).long().cuda()\n",
    "    segment_indices = torch.IntTensor(segment_indices).cuda()\n",
    "    \n",
    "    return input_frames, labels, segment_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, folder):\n",
    "    filename = folder + 'model_checkpoint.pth'\n",
    "    torch.save(state, filename)\n",
    "    \n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, folder + 'model_best.pth')\n",
    "\n",
    "def train_model(model, num_epochs=10, train_data_file=TRAINING_DATA, validation_data_file=VALIDATION_DATA):\n",
    "    training_losses = []\n",
    "    validation_losses = []\n",
    "    training_accuracies = []\n",
    "    validation_accuracies = []\n",
    "    best = float(\"inf\")\n",
    "\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        start = time.time()\n",
    "        print('======================================================================')\n",
    "        print('Epoch: {}'.format(epoch))\n",
    "\n",
    "        try:\n",
    "            train_f.close()    \n",
    "        except(NameError):\n",
    "            pass   \n",
    "        \n",
    "        train_f = open(train_data_file, 'rb')\n",
    "        should_epoch_end = False\n",
    "        \n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        video_num = 0\n",
    "        epoch_training_losses = []\n",
    "\n",
    "        model.train()\n",
    "        while not should_epoch_end:\n",
    "            video, should_epoch_end = get_next_video_data(train_f)\n",
    "            if should_epoch_end:\n",
    "                break\n",
    "            \n",
    "            inputs, labels, segment_indices = transform_to_inputs(video)\n",
    "\n",
    "            outputs = model(inputs, segment_indices)\n",
    "            loss = loss_criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_training_losses.append(loss.item())\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            video_num += 1\n",
    "            if video_num % 150 == 0:\n",
    "                print('    Video: {}/1168  Loss: {}'.format(video_num, np.mean(epoch_training_losses))) \n",
    "\n",
    "        train_f.close()            \n",
    "        \n",
    "        train_accuracy = 100 * correct / total\n",
    "        \n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        try:\n",
    "            val_f.close()    \n",
    "        except(NameError):\n",
    "            pass  \n",
    "            \n",
    "        val_f = open(validation_data_file, 'rb')\n",
    "        model.eval()\n",
    "\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        is_file_end = False\n",
    "\n",
    "        epoch_validation_losses = []\n",
    "        while not is_file_end:\n",
    "            video, is_file_end = get_next_video_data(val_f)\n",
    "            if is_file_end:\n",
    "                break\n",
    "            \n",
    "            inputs, labels, segment_indices = transform_to_inputs(video)\n",
    "        \n",
    "            outputs = model(inputs, segment_indices)\n",
    "            loss = loss_criterion(outputs, labels)\n",
    "            epoch_validation_losses.append(loss.item())\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_f.close()\n",
    "        \n",
    "        val_accuracy = 100 * correct / total\n",
    "        \n",
    "        \n",
    "        # Print loss and accuracy\n",
    "        training_losses.append(np.mean(epoch_training_losses))\n",
    "        validation_losses.append(np.mean(epoch_validation_losses))\n",
    "        training_accuracies.append(train_accuracy)\n",
    "        validation_accuracies.append(val_accuracy)\n",
    "        \n",
    "\n",
    "        scheduler.step(validation_losses[-1])\n",
    "\n",
    "        end = time.time()\n",
    "\n",
    "        print(' Training Loss: {:.5f}  Validation Loss: {:.5f}'.format(training_losses[-1], validation_losses[-1]))\n",
    "        print(' Training Accuracy: {:.3f}%   Validation Accuracy: {:.3f}%'.format(train_accuracy, val_accuracy))        \n",
    "        \n",
    "        is_best = validation_losses[-1] < best\n",
    "        best = min(validation_losses[-1], best)\n",
    "        save_checkpoint({\n",
    "            'next_epoch_idx': epoch + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'val_loss': validation_losses[-1],\n",
    "            'optimizer' : optimizer.state_dict(),\n",
    "        }, is_best, RESULTS_DIR + model_time + '/')\n",
    "\n",
    "        print('Best Validation Loss: {:.5f}'.format(best))\n",
    "        print('Epoch took {:.2f} minutes'.format((end - start) / 60))\n",
    "\n",
    "    return training_losses, validation_losses, training_accuracies, validation_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Epoch: 0\n",
      "    Video: 150/1168  Loss: 3.8083478167043254\n",
      "    Video: 300/1168  Loss: 3.6581033861031536\n",
      "    Video: 450/1168  Loss: 3.56183340990323\n",
      "    Video: 600/1168  Loss: 3.5107208189650994\n",
      "    Video: 750/1168  Loss: 3.443489384116048\n",
      "    Video: 900/1168  Loss: 3.370962505415697\n",
      "    Video: 1050/1168  Loss: 3.3083165757360464\n",
      " Training Loss: 3.26858  Validation Loss: 2.66405\n",
      " Training Accuracy: 12.710%   Validation Accuracy: 24.683%\n",
      "Best Validation Loss: 2.66405\n",
      "Epoch took 6.43 minutes\n",
      "======================================================================\n",
      "Epoch: 1\n",
      "    Video: 150/1168  Loss: 2.6233020719844853\n",
      "    Video: 300/1168  Loss: 2.635120172767075\n",
      "    Video: 450/1168  Loss: 2.590397268307727\n",
      "    Video: 600/1168  Loss: 2.57795529254681\n"
     ]
    }
   ],
   "source": [
    "model_time = str(datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))\n",
    "os.mkdir(RESULTS_DIR + model_time)\n",
    "\n",
    "## Model Architecture\n",
    "input_dim = 400  # dimension of an i3D video frame\n",
    "hidden_dim = 80 # dimension of hidden state\n",
    "layer_dim = 1    # number of stacked layers\n",
    "output_dim = 48  # number of sub-action labels\n",
    "\n",
    "\n",
    "# model = BiLSTM(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "model = BiGRU(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "\n",
    "model = model.double().cuda() # transform the model parameters to double precision\n",
    "\n",
    "\n",
    "## Loss function\n",
    "loss_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "## Optimizer\n",
    "learning_rate = 0.01\n",
    "weight_decay = 0.0002\n",
    "momentum = 0.9\n",
    "\n",
    "# optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay, momentum=momentum)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay) \n",
    "optimizer = optim.Adagrad(model.parameters(), lr=learning_rate, weight_decay=weight_decay) \n",
    "\n",
    "\n",
    "## Learning Rate Scheduler\n",
    "patience = 2\n",
    "decrease_factor = 0.7\n",
    "min_learning_rate = 0.00005\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', \n",
    "                              patience=patience, min_lr=min_learning_rate, factor=decrease_factor,\n",
    "                              verbose=True)\n",
    "\n",
    "\n",
    "num_epochs = 25\n",
    "\n",
    "training_losses, validation_losses, training_accuracies, validation_accuracies = train_model(model,num_epochs=num_epochs) # TODO: batch_sizes\n",
    "\n",
    "path = RESULTS_DIR + model_time + \"/bidirectional_video_model.pth\"\n",
    "torch.save(model.state_dict(), path)\n",
    "print(\"\\nSaved trained model to\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation losses\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.title(\"Average batch Training and validation loss in each epoch\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(training_losses)\n",
    "plt.plot(validation_losses)\n",
    "plt.legend([\"Training loss\",\"Validation loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation accuracies\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.title(\"Training and validation accuracy in each epoch\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(training_accuracies)\n",
    "plt.plot(validation_accuracies)\n",
    "plt.legend([\"Training accuracy\",\"Validation accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load saved checkpoint\n",
    "# checkpoint_path = RESULTS_DIR + \"2020-03-21-_04-30-14\" + \"/model_checkpoint.pth\"\n",
    "# checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "# next_epoch_idx = checkpoint['next_epoch_idx']\n",
    "# val_loss = checkpoint['val_loss']\n",
    "# model.load_state_dict(checkpoint['state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "# print(\"=> loaded checkpoint '{}' (next_epoch_idx {})\".format(checkpoint_path, next_epoch_idx))\n",
    "\n",
    "# # # Load saved model\n",
    "# # path = RESULTS_DIR + \"2020-03-21-_04-30-14\" + \"/bilstm_video_model.pth\"\n",
    "# # model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "try:\n",
    "    val_f.close()    \n",
    "except(NameError):\n",
    "    ;   \n",
    "val_f = open(VALIDATION_DATA, 'rb')\n",
    "\n",
    "y = {'Actual': [], 'Predicted': []}\n",
    "\n",
    "is_file_end = False\n",
    "while not is_file_end:\n",
    "    video, is_file_end = get_next_video_data(val_f)\n",
    "    if is_file_end:\n",
    "        break\n",
    "\n",
    "    inputs, labels, segment_indices = transform_to_inputs(video)\n",
    "\n",
    "    outputs = model(inputs, segment_indices)\n",
    "\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    \n",
    "    y['Actual'].extend(labels.tolist())\n",
    "    y['Predicted'].extend(predicted.tolist())\n",
    "    \n",
    "y_df = pd.DataFrame(y)\n",
    "path = RESULTS_DIR + model_time + \"/val_results.csv\"        \n",
    "y_df.to_csv(path, encoding='utf-8', index=False)\n",
    "\n",
    "print('Done predicting on validation set. Saved to', path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, roc_auc_score\n",
    "\n",
    "print('\\nMacro f1 Score= %.4f' % f1_score(y['Actual'], y['Predicted'], average=\"macro\"))\n",
    "print('Macro Precision= %.4f' % precision_score(y['Actual'], y['Predicted'], zero_division=0, average=\"macro\"))\n",
    "print('Macro Recall= %.4f' % recall_score(y['Actual'], y['Predicted'], average=\"macro\")) \n",
    "\n",
    "print('\\nMicro f1 Score= %.4f' % f1_score(y['Actual'], y['Predicted'], average=\"micro\"))\n",
    "print('Micro Precision= %.4f' % precision_score(y['Actual'], y['Predicted'], zero_division=0, average=\"micro\"))\n",
    "print('Micro Recall= %.4f' % recall_score(y['Actual'], y['Predicted'], average=\"micro\")) \n",
    "\n",
    "print('\\nAccuracy: %.4f' % accuracy_score(y['Actual'], y['Predicted']))\n",
    "\n",
    "# Computes the average AUC of all possible pairwise combinations of classes. \n",
    "# Insensitive to class imbalance when average == 'macro'.\n",
    "# print(roc_auc_score(y['Actual'], y['Predicted'], multi_class='ovo', average='macro')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate test data predictions for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TEST_SEGMENTS = 1284\n",
    "\n",
    "model.eval()\n",
    "\n",
    "try:\n",
    "    test_f.close()    \n",
    "except(NameError):\n",
    "    pass   \n",
    "test_f = open(TEST_DATA, 'rb')\n",
    "\n",
    "y_pred = {'Id': np.arange(NUM_TEST_SEGMENTS), 'Category': []}\n",
    "\n",
    "def transform_to_test_inputs(video):\n",
    "    segments = [] # segments (list of frames) in the video\n",
    "    segment_indices = []\n",
    "    offset = 0\n",
    "    for segment_num in range(len(video)):\n",
    "        segments.append(video[segment_num])\n",
    "        segment_indices.append((offset, offset + video[segment_num].shape[0]))\n",
    "\n",
    "        offset += video[segment_num].shape[0]\n",
    "\n",
    "    # Load frames as tensors with gradient accumulation abilities\n",
    "    input_frames = torch.cat(segments, 0).unsqueeze(0).requires_grad_().cuda() # unsqueeze to add batch dim\n",
    "    segment_indices = torch.IntTensor(segment_indices).cuda()\n",
    "    \n",
    "    return input_frames, segment_indices\n",
    "        \n",
    "is_end_reached = False\n",
    "while not is_end_reached:\n",
    "    video, is_end_reached = get_next_video_data(test_f)\n",
    "\n",
    "    if is_end_reached:\n",
    "        break\n",
    "\n",
    "    inputs, segment_indices = transform_to_test_inputs(video)\n",
    "    \n",
    "    outputs = model(inputs, segment_indices)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    \n",
    "    y_pred['Category'].extend(predicted.tolist())\n",
    "\n",
    "y_pred_df = pd.DataFrame(y_pred)\n",
    "path = RESULTS_DIR + model_time + \"/\" + model_time + \"_predictions_submission.csv\"\n",
    "y_pred_df.to_csv(path, encoding='utf-8', index=False)\n",
    "\n",
    "print('Done predicting on test data. Saved to', path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
