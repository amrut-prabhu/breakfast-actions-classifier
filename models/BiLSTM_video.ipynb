{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import _pickle as pickle\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "import time\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = \"../\"\n",
    "\n",
    "DATA_DIR = ROOT_DIR\n",
    "RESULTS_DIR = ROOT_DIR + \"results/\"\n",
    "\n",
    "# Train - 1460 (1168 + 292) videos \n",
    "# Test - 252 videos \n",
    "\n",
    "TRAINING_DATA = DATA_DIR + \"video_training_data.p\" # each read gives labelled video [(segment, label), segment_2, ...]\n",
    "VALIDATION_DATA = DATA_DIR + \"video_validation_data.p\" \n",
    "TEST_DATA = DATA_DIR + \"video_testing_data.p\" # each read gives video [(segment), segment_2, ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def transform_input(segments):\n",
    "#     \"\"\"\n",
    "#     Transform input with zero padding so that all input segments \n",
    "#     in the batch have same size.\n",
    "#     \"\"\"\n",
    "#     input_dim = 400 # num features in an i3D frame\n",
    "#     segment_lengths = [len(segments) for segments in segments]\n",
    "    \n",
    "#     # Create an empty matrix with zero padding\n",
    "#     longest_segment = max(segment_lengths)\n",
    "#     batch_size = len(segments)\n",
    "#     padded_segments = np.zeros((batch_size, longest_segment, input_dim))\n",
    "    \n",
    "#     # Copy over the actual frame sequences\n",
    "#     for i, length in enumerate(segment_lengths):\n",
    "#         sequence = segments[i]\n",
    "#         padded_segments[i, 0:length] = sequence[:length]\n",
    "    \n",
    "#     # Change back to tensor of type double\n",
    "#     transformed_segments = []\n",
    "#     for padded_segment in padded_segments:\n",
    "#         transformed_segments.append(torch.Tensor(padded_segment).double())\n",
    "    \n",
    "#     return transformed_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_next_data_batch(f, batch_size):    \n",
    "#     raw_videos = []\n",
    "#     is_end_reached = False\n",
    "    \n",
    "#     num_videos = 0\n",
    "#     while num_videos < batch_size:\n",
    "#         try:\n",
    "#             video = pickle.load(f) # [(segment, label), ...]\n",
    "            \n",
    "#             raw_videos.append(video)\n",
    "#             num_videos += 1\n",
    "#         except (EOFError):\n",
    "#             is_end_reached = True\n",
    "#             break\n",
    "    \n",
    "#     if len(raw_videos) == 0:\n",
    "#         if not is_end_reached:\n",
    "#             print('Returning empty data, but end of file NOT reached')\n",
    "#         return [], [], is_end_reached\n",
    "        \n",
    "# #     How to pad video??\n",
    "# #     padded_segments = transform_input(raw_segments)\n",
    "# #     segments = torch.stack(padded_segments).double()\n",
    "\n",
    "#     return raw_videos, is_end_reached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_video_data(f):    \n",
    "    is_end_reached = False\n",
    "    \n",
    "    try:\n",
    "        video = pickle.load(f) # [(segment, label), ...]\n",
    "    except (EOFError):\n",
    "        is_end_reached = True\n",
    "        return [], is_end_reached\n",
    "    \n",
    "    return video, is_end_reached"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassPredictor(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(ClassPredictor, self).__init__()\n",
    "        \n",
    "        hidden_1 = 100\n",
    "        hidden_2 = 40\n",
    "        hidden_3 = 15\n",
    "        \n",
    "        self.temp = nn.Linear(input_size, num_classes)\n",
    "        \n",
    "        # https://pytorch.org/docs/stable/nn.html#torch.nn.Linear\n",
    "        self.fc1 = nn.Linear(input_size, hidden_1)\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=hidden_1)\n",
    "        self.l_relu1 = nn.LeakyReLU()\n",
    "        self.dout1 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        self.bn2 = nn.BatchNorm1d(num_features=hidden_2)\n",
    "        self.l_relu2 = nn.LeakyReLU()\n",
    "        self.dout2 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.fc3 = nn.Linear(hidden_2, hidden_3)\n",
    "        self.bn3 = nn.BatchNorm1d(num_features=hidden_3)\n",
    "        self.l_relu3 = nn.LeakyReLU()\n",
    "        \n",
    "        self.out = nn.Linear(hidden_3, num_classes)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            nn.init.xavier_uniform_(self.fc1.weight, gain=nn.init.calculate_gain('leaky_relu'))\n",
    "            nn.init.xavier_uniform_(self.fc2.weight, gain=nn.init.calculate_gain('leaky_relu'))\n",
    "            nn.init.xavier_uniform_(self.fc3.weight, gain=nn.init.calculate_gain('relu'))\n",
    "            nn.init.xavier_uniform_(self.out.weight)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        # x: (input_size)\n",
    "        return self.temp(x)\n",
    "        \n",
    "        a1 = self.fc1(x)\n",
    "#         b1 = self.bn1(a1)\n",
    "        h1 = self.l_relu1(a1)\n",
    "        \n",
    "        dout1 = self.dout1(h1)\n",
    "        a2 = self.fc2(dout1)\n",
    "#         b2 = self.bn2(a2)\n",
    "        h2 = self.l_relu2(a2)\n",
    "        \n",
    "        dout2 = self.dout2(h2)\n",
    "        a3 = self.fc3(dout2)\n",
    "#         b3 = self.bn3(a3)\n",
    "        h3 = self.l_relu3(a3)\n",
    "        \n",
    "        # y: (num_classes)\n",
    "        y = self.out(h3)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bidirectional recurrent neural network (many-to-one)\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # https://pytorch.org/docs/stable/nn.html#torch.nn.LSTM\n",
    "        self.bilstm = nn.LSTM(input_size, hidden_size, num_layers, bidirectional=True, \n",
    "                            batch_first=True, bias=True, dropout=0)\n",
    "        \n",
    "        # DNN for class prediction\n",
    "        self.fc = ClassPredictor(hidden_size * 2, num_classes)\n",
    " \n",
    "\n",
    "    def init_hidden_state(self, batch_size):\n",
    "        #h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size)\n",
    "        #c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size)\n",
    "\n",
    "        # https://pytorch.org/docs/stable/nn.init.html\n",
    "\n",
    "        h0 = torch.empty(self.num_layers * 2, batch_size, self.hidden_size).double()\n",
    "        nn.init.orthogonal_(h0) # orthogonal_, xavier_normal_, xavier_uniform_\n",
    "        \n",
    "        c0 = torch.empty(self.num_layers * 2, batch_size, self.hidden_size).double()\n",
    "        nn.init.orthogonal_(c0) # orthogonal_, xavier_normal_, xavier_uniform_\n",
    "\n",
    "        h0.requires_grad_()#.to(device)\n",
    "        c0.requires_grad_()#.to(device)\n",
    "        \n",
    "        return h0, c0\n",
    " \n",
    "\n",
    "    def forward(self, x, segment_indices):\n",
    "        # x: (batch_size, seq_len, feature_len)\n",
    "        # segment_indices: (num_segments, 2)\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Set initial states\n",
    "        # h0, c0: (num_layers * num_directions, batch_size, hidden_size)\n",
    "        h0, c0 = self.init_hidden_state(batch_size)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        # out: tensor of shape (batch_size, seq_length, hidden_size * 2)\n",
    "        out, _ = self.bilstm(x, (h0, c0))  \n",
    "        \n",
    "        # Use hidden states of each segment to predict their labels\n",
    "        segment_outputs = []\n",
    "        for (start, end) in segment_indices:\n",
    "            hidden_states = out[:, start:end, :]\n",
    "            # Compute the hidden state by doing temporal pooling over all time steps\n",
    "            # pool_out: (hidden_size * 2)\n",
    "\n",
    "            # TODO: concat pooling.\n",
    "            # https://medium.com/@sonicboom8/sentiment-analysis-with-variable-length-sequences-in-pytorch-6241635ae130\n",
    "            max_pool_out = F.adaptive_max_pool1d(hidden_states.permute(0,2,1), 1).squeeze()\n",
    "\n",
    "    #         avg_pool_out = F.adaptive_avg_pool1d(out.permute(0,2,1), 1).squeeze()\n",
    "    #         final_hidden_out = out[:,-1, :]\n",
    "            \n",
    "            # output: (num_classes)\n",
    "            output = self.fc(max_pool_out)\n",
    "            \n",
    "            segment_outputs.append(output)\n",
    "\n",
    "        \n",
    "        # segment_outputs: (num_segments, num_classes)\n",
    "        segment_outputs = torch.stack(segment_outputs)\n",
    "        \n",
    "        return segment_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model on entire videos (sequence of segments which are sequence of frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_inputs(video):\n",
    "    segments = [] # segments (list of frames) in the video\n",
    "    labels = [] # labels of each segment\n",
    "    segment_indices = []\n",
    "    offset = 0\n",
    "    for segment_num in range(len(video)):\n",
    "        segments.append(video[segment_num][0])\n",
    "        labels.append(video[segment_num][1] - 1) # NOTE: subtract 1 since SIL (class 0) is not used\n",
    "        segment_indices.append((offset, offset + video[segment_num][0].shape[0]))\n",
    "\n",
    "        offset += video[segment_num][0].shape[0]\n",
    "        \n",
    "    # Load frames as tensors with gradient accumulation abilities\n",
    "    input_frames = torch.cat(segments, 0).unsqueeze(0).requires_grad_() # unsqueeze to add batch dim\n",
    "    labels = torch.Tensor(labels).long()\n",
    "    \n",
    "    return input_frames, labels, segment_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, filename):\n",
    "    torch.save(state, filename)\n",
    "\n",
    "def train_model(model, num_epochs=10, train_data_file=TRAINING_DATA, validation_data_file=VALIDATION_DATA):\n",
    "    training_losses = []\n",
    "    validation_losses = []\n",
    "\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        start = time.time()\n",
    "        print('======================================================================')\n",
    "        print('Epoch: {}'.format(epoch))\n",
    "\n",
    "        try:\n",
    "            train_f.close()    \n",
    "        except(NameError):\n",
    "            ;   \n",
    "        \n",
    "        train_f = open(train_data_file, 'rb')\n",
    "        should_epoch_end = False\n",
    "\n",
    "        video_num = 0\n",
    "        epoch_training_losses = []\n",
    "\n",
    "        model.train()\n",
    "        while not should_epoch_end:\n",
    "            video, should_epoch_end = get_next_video_data(train_f)\n",
    "            if should_epoch_end:\n",
    "                break\n",
    "            \n",
    "            inputs, labels, segment_indices = transform_to_inputs(video)\n",
    "\n",
    "            outputs = model(inputs, segment_indices)\n",
    "            loss = loss_criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_training_losses.append(loss.item())\n",
    "\n",
    "            video_num += 1\n",
    "            if video_num % 200 == 0:\n",
    "                print('    Iteration: {}. Loss: {}.'.format(iter, loss.item())) \n",
    "\n",
    "    \n",
    "        train_f.close()            \n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        try:\n",
    "            val_f.close()    \n",
    "        except(NameError):\n",
    "            ;  \n",
    "            \n",
    "        val_f = open(validation_data_file, 'rb')\n",
    "        model.eval()\n",
    "\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        is_file_end = False\n",
    "\n",
    "        epoch_validation_losses = []\n",
    "        while not is_file_end:\n",
    "            video, is_file_end = get_next_video_data(train_f)\n",
    "            if is_file_end:\n",
    "                break\n",
    "            \n",
    "            inputs, labels, segment_indices = transform_to_inputs(video)\n",
    "        \n",
    "            outputs = model(inputs, segment_indices)\n",
    "            loss = loss_criterion(outputs, labels)\n",
    "            epoch_validation_losses.append(loss.item())\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_f.close()\n",
    "        \n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        \n",
    "        # Print loss\n",
    "        training_losses.append(np.mean(epoch_training_losses))\n",
    "        validation_losses.append(np.mean(epoch_validation_losses))\n",
    "\n",
    "        scheduler.step(validation_losses[-1])\n",
    "\n",
    "        print(f' Training Loss: {training_losses[-1]} Validation Loss: {validation_losses[-1]} Validation Accuracy: {accuracy}%')\n",
    "        end = time.time()\n",
    "        print(f'Epoch took {end - start} seconds')\n",
    "        \n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer' : optimizer.state_dict(),\n",
    "        }, RESULTS_DIR + model_time + '/model_checkpoint.pth')\n",
    "        \n",
    "    return training_losses, validation_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Epoch: 0\n"
     ]
    }
   ],
   "source": [
    "model_time = str(datetime.now().strftime(\"%Y-%m-%d_%I-%M-%S\"))\n",
    "os.mkdir(model_time)\n",
    "\n",
    "## Segment (inner) Model Architecture\n",
    "input_dim = 400  # dimension of an i3D video frame\n",
    "hidden_dim = 100 # dimension of hidden state\n",
    "layer_dim = 1    # number of stacked layers\n",
    "output_dim = 47  # number of sub-action labels\n",
    "\n",
    "\n",
    "model = BiLSTM(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "model = model.double() # transform the model parameters to double precision\n",
    "\n",
    "\n",
    "## Loss function\n",
    "loss_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "## Optimizer\n",
    "learning_rate = 0.01\n",
    "weight_decay = 0.002\n",
    "momentum = 0.9\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay, momentum=momentum)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay) \n",
    "\n",
    "\n",
    "## Learning Rate Scheduler\n",
    "patience = 3\n",
    "decrease_factor = 0.5\n",
    "min_learning_rate = 0.00005\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', \n",
    "                              patience=patience, min_lr=min_learning_rate, factor=decrease_factor,\n",
    "                              verbose=True)\n",
    "\n",
    "\n",
    "num_epochs = 3\n",
    "\n",
    "training_losses, validation_losses = train_model(model,num_epochs=num_epochs) # TODO: batch_sizes\n",
    "\n",
    "path = RESULTS_DIR + model_time + \"/rnn_model.pth\"\n",
    "torch.save(model.state_dict(), path)\n",
    "print(\"\\nSaved trained model to\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation losses\n",
    "# training_losses = training_losses.mean()\n",
    "# validation_losses = validation_losses.mean()\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.title(\"Average batch Training and validation loss in each epoch\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(training_losses)\n",
    "plt.plot(validation_losses)\n",
    "plt.legend([\"Training loss\",\"Validation loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load saved checkpoint\n",
    "# checkpoint_path = RESULTS_DIR + \"2020-03-21-_04-30-14\" + \"/model_checkpoint.pth\"\n",
    "# checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "# epoch = checkpoint['epoch']\n",
    "# model.load_state_dict(checkpoint['state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "# print(\"=> loaded checkpoint '{}' (epoch {})\".format(checkpoint_path, epoch))\n",
    "\n",
    "# # # Load saved model\n",
    "# # path = RESULTS_DIR + \"2020-03-21-_04-30-14\" + \"/rnn_model.pth\"\n",
    "# # model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "try:\n",
    "    val_f.close()    \n",
    "except(NameError):\n",
    "    ;   \n",
    "val_f = open(VALIDATION_DATA, 'rb')\n",
    "\n",
    "y = {'Actual': [], 'Predicted': []}\n",
    "\n",
    "is_end_reached = False\n",
    "while not is_end_reached:\n",
    "    segments, labels, is_end_reached = get_next_data_batch(val_f, 1)\n",
    "    \n",
    "    if len(segments) == 0:\n",
    "        break\n",
    "    \n",
    "    outputs = model(segments)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    \n",
    "    y['Actual'].extend(labels.tolist())\n",
    "    y['Predicted'].extend(predicted.tolist())\n",
    "    \n",
    "y_df = pd.DataFrame(y)\n",
    "path = RESULTS_DIR + model_time + \"/prediction_results.csv\"        \n",
    "y_df.to_csv(path, encoding='utf-8', index=False)\n",
    "\n",
    "print('Done predicting on validation set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, roc_auc_score\n",
    "\n",
    "print('\\nMacro f1 Score= %.4f' % f1_score(y['Actual'], y['Predicted'], average=\"macro\"))\n",
    "print('Macro Precision= %.4f' % precision_score(y['Actual'], y['Predicted'], zero_division=0, average=\"macro\"))\n",
    "print('Macro Recall= %.4f' % recall_score(y['Actual'], y['Predicted'], average=\"macro\")) \n",
    "\n",
    "print('\\nMicro f1 Score= %.4f' % f1_score(y['Actual'], y['Predicted'], average=\"micro\"))\n",
    "print('Micro Precision= %.4f' % precision_score(y['Actual'], y['Predicted'], zero_division=0, average=\"micro\"))\n",
    "print('Micro Recall= %.4f' % recall_score(y['Actual'], y['Predicted'], average=\"micro\")) \n",
    "\n",
    "print('\\nAccuracy: %.4f' % accuracy_score(y['Actual'], y['Predicted']))\n",
    "\n",
    "# Computes the average AUC of all possible pairwise combinations of classes. \n",
    "# Insensitive to class imbalance when average == 'macro'.\n",
    "# print(roc_auc_score(y['Actual'], y['Predicted'], multi_class='ovo', average='macro')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate test data predictions for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TEST_SEGMENTS = 1284\n",
    "\n",
    "def get_next_test_batch(f, batch_size=1):    \n",
    "    raw_segments = []\n",
    "    \n",
    "    is_end_reached = False\n",
    "    \n",
    "    num_segments = 0\n",
    "    while num_segments < batch_size:\n",
    "        try:\n",
    "            segment = pickle.load(f)\n",
    "            \n",
    "            raw_segments.append(segment)\n",
    "            num_segments += 1\n",
    "        except (EOFError):\n",
    "            is_end_reached = True\n",
    "            break\n",
    "\n",
    "    padded_segments = transform_input(raw_segments)\n",
    "    segments = torch.stack(padded_segments).double()\n",
    "    \n",
    "    return segments, is_end_reached\n",
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "try:\n",
    "    test_f.close()    \n",
    "except(NameError):\n",
    "    ;   \n",
    "test_f = open(TEST_DATA, 'rb')\n",
    "\n",
    "y_pred = {'Id': np.arange(NUM_TEST_SEGMENTS), 'Category': []}\n",
    "\n",
    "# https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/02-intermediate/bidirectional_recurrent_neural_network/main.py\n",
    "\n",
    "is_end_reached = False\n",
    "while not is_end_reached:\n",
    "    segments, is_end_reached = get_next_test_batch(test_f, 50)\n",
    "\n",
    "    outputs = model(segments)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    \n",
    "    y_pred['Category'].extend(predicted.tolist())\n",
    "\n",
    "y_pred_df = pd.DataFrame(y_pred)\n",
    "path = RESULTS_DIR + model_time + \"/predictions_submission.csv\"\n",
    "y_pred_df.to_csv(path, encoding='utf-8', index=False)\n",
    "\n",
    "print('Done predicting on test data')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
